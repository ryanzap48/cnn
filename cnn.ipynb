{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'{sys.executable}' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "pkgs = [\n",
    "    \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\"\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"\\nInstalling {p} ...\")\n",
    "    !{sys.executable} -m pip install {p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA GeForce RTX 2070 SUPER\n",
      "GPU memory: 8.00 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 189,932 interactions\n",
      "Created 104 unique templates\n",
      "\n",
      "Direction distribution:\n",
      "Direction\n",
      "increased    128502\n",
      "decreased     61399\n",
      "mixed            31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Created 60 base categories\n",
      "Combined into 75 final categories\n",
      "\n",
      "================================================================================\n",
      "✓ Number of classes for training: 75\n",
      "================================================================================\n",
      "\n",
      "FINAL CLASS LABELS (with direction):\n",
      "================================================================================\n",
      "    0. [↓] absorption_change............................     45 samples\n",
      "    1. [↑] adverse_effects_general...................... 60,010 samples\n",
      "    2. [↑] adverse_neuromuscular........................     94 samples\n",
      "    3. [↓] analgesic_effect.............................     10 samples\n",
      "    4. [↑] analgesic_effect.............................    270 samples\n",
      "    5. [↑] anticholinergic_effect.......................    323 samples\n",
      "    6. [↓] anticoagulation..............................    225 samples\n",
      "    7. [↑] anticoagulation..............................  3,055 samples\n",
      "    8. [↓] antiplatelet_effect..........................      7 samples\n",
      "    9. [↑] antiplatelet_effect..........................    336 samples\n",
      "   10. [↑] antipsychotic_effect.........................     94 samples\n",
      "   11. [↑] arrhythmia...................................    349 samples\n",
      "   12. [↑] av_block.....................................    716 samples\n",
      "   13. [↓] bioavailability_change.......................    518 samples\n",
      "   14. [↑] bioavailability_change.......................     11 samples\n",
      "   15. [↑] bleeding_risk................................    126 samples\n",
      "   16. [↑] bradycardia..................................  1,276 samples\n",
      "   17. [↑] bronchoconstriction..........................     26 samples\n",
      "   18. [↓] bronchodilation..............................    361 samples\n",
      "   19. [↓] cardiotoxicity...............................  1,033 samples\n",
      "   20. [↑] cardiotoxicity...............................    200 samples\n",
      "   21. [↑] cns_depression...............................  5,391 samples\n",
      "   22. [↑] constipation.................................    146 samples\n",
      "   23. [↑] dermatologic_adverse.........................     11 samples\n",
      "   24. [↓] diagnostic_interference......................     33 samples\n",
      "   25. [↓] diuretic_effect..............................    324 samples\n",
      "   26. [↓] excretion_change.............................  1,815 samples\n",
      "   27. [±] excretion_change.............................     31 samples\n",
      "   28. [↑] fluid_retention..............................    422 samples\n",
      "   29. [↑] heart_failure................................     26 samples\n",
      "   30. [↑] hepatotoxicity...............................     52 samples\n",
      "   31. [↑] hypercalcemia................................     76 samples\n",
      "   32. [↑] hyperglycemia................................     28 samples\n",
      "   33. [↑] hyperkalemia.................................    284 samples\n",
      "   34. [↑] hypersensitivity.............................     26 samples\n",
      "   35. [↓] hypertension.................................  3,085 samples\n",
      "   36. [↑] hypertension.................................  1,317 samples\n",
      "   37. [↑] hypocalcemia.................................    180 samples\n",
      "   38. [↑] hypoglycemia.................................  2,109 samples\n",
      "   39. [↑] hypokalemia..................................  1,130 samples\n",
      "   40. [↑] hyponatremia.................................    106 samples\n",
      "   41. [↑] hypotension..................................  8,160 samples\n",
      "   42. [↑] immunosuppression............................    312 samples\n",
      "   43. [↓] metabolism_change............................ 34,295 samples\n",
      "   44. [↑] metabolism_change............................  5,000 samples\n",
      "   45. [↓] metabolite_concentration_change..............    311 samples\n",
      "   46. [↑] metabolite_concentration_change..............    537 samples\n",
      "   47. [↑] myelosuppression.............................     34 samples\n",
      "   48. [↑] nephrotoxicity...............................    660 samples\n",
      "   49. [↑] neuroexcitation..............................    936 samples\n",
      "   50. [↓] neuromuscular_blockade.......................     79 samples\n",
      "   51. [↑] neuromuscular_blockade.......................    392 samples\n",
      "   52. [↑] neurotoxicity................................     82 samples\n",
      "   53. [↑] orthostatic_hypotension......................    601 samples\n",
      "   54. [↑] other_interaction............................     45 samples\n",
      "   55. [↑] ototoxicity..................................     21 samples\n",
      "   56. [↓] protein_binding_change.......................     11 samples\n",
      "   57. [↑] qtc_prolongation.............................  6,580 samples\n",
      "   58. [↑] respiratory_depression.......................    280 samples\n",
      "   59. [↑] rhabdomyolysis...............................     69 samples\n",
      "   60. [↓] sedation.....................................    558 samples\n",
      "   61. [↑] sedation.....................................  1,007 samples\n",
      "   62. [↑] serotonergic_effect..........................    792 samples\n",
      "   63. [↓] serum_concentration_change................... 10,223 samples\n",
      "   64. [↑] serum_concentration_change................... 23,775 samples\n",
      "   65. [↓] stimulation..................................    498 samples\n",
      "   66. [↑] stimulation..................................     56 samples\n",
      "   67. [↑] tachycardia..................................    372 samples\n",
      "   68. [↓] therapeutic_efficacy.........................  7,659 samples\n",
      "   69. [↑] therapeutic_efficacy.........................    243 samples\n",
      "   70. [↑] thrombosis...................................    108 samples\n",
      "   71. [↑] ulcerogenic_effect...........................     43 samples\n",
      "   72. [↓] vasoconstriction.............................    309 samples\n",
      "   73. [↑] vasoconstriction.............................    174 samples\n",
      "   74. [↑] vasodilation.................................     33 samples\n",
      "\n",
      "✓ Saved to 'drug_interactions_categorized.csv'\n",
      "✓ Saved label encoder\n",
      "✓ Saved label mapping to 'label_mapping.txt'\n",
      "\n",
      "================================================================================\n",
      "Next step: Run your fingerprint generation script!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_direction(description):\n",
    "    \"\"\"\n",
    "    Determine if the interaction increases or decreases the effect\n",
    "    \"\"\"\n",
    "    desc_lower = description.lower()\n",
    "    \n",
    "    # Check for increase variations\n",
    "    increase_patterns = [\n",
    "        r'\\bincreas(e|ed|es|ing)\\b',\n",
    "        r'\\b(enhance|enhanced|enhances|enhancing)\\b',\n",
    "        r'\\b(elevat(e|ed|es|ing)|rais(e|ed|es|ing))\\b',\n",
    "        r'\\bpotentiat(e|ed|es|ing)\\b',\n",
    "        r'\\bamplif(y|ied|ies|ying)\\b'\n",
    "    ]\n",
    "    \n",
    "    # Check for decrease variations\n",
    "    decrease_patterns = [\n",
    "        r'\\bdecreas(e|ed|es|ing)\\b',\n",
    "        r'\\b(reduc(e|ed|es|ing)|diminish(ed|es|ing)?)\\b',\n",
    "        r'\\b(lower(ed|s|ing)?)\\b',\n",
    "        r'\\battenuat(e|ed|es|ing)\\b',\n",
    "        r'\\bweaken(ed|s|ing)?\\b',\n",
    "        r'\\blessen(ed|s|ing)?\\b'\n",
    "    ]\n",
    "    \n",
    "    has_increase = any(re.search(pattern, desc_lower) for pattern in increase_patterns)\n",
    "    has_decrease = any(re.search(pattern, desc_lower) for pattern in decrease_patterns)\n",
    "    \n",
    "    # Handle cases with both (should be rare)\n",
    "    if has_increase and has_decrease:\n",
    "        # Count occurrences to determine dominant direction\n",
    "        increase_count = sum(len(re.findall(pattern, desc_lower)) for pattern in increase_patterns)\n",
    "        decrease_count = sum(len(re.findall(pattern, desc_lower)) for pattern in decrease_patterns)\n",
    "        \n",
    "        if increase_count > decrease_count:\n",
    "            return 'increased'\n",
    "        elif decrease_count > increase_count:\n",
    "            return 'decreased'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "    elif has_increase:\n",
    "        return 'increased'\n",
    "    elif has_decrease:\n",
    "        return 'decreased'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "\n",
    "def categorize_template(template):\n",
    "    \"\"\"\n",
    "    Categorize interaction templates into high-level classes\n",
    "    \"\"\"\n",
    "    template_lower = template.lower()\n",
    "    \n",
    "    # 1. ADVERSE EFFECTS / TOXICITY (General)\n",
    "    if 'adverse effects' in template_lower and 'risk or severity' in template_lower:\n",
    "        return 'adverse_effects_general'\n",
    "    \n",
    "    # 2. SPECIFIC TOXICITIES\n",
    "    if 'cardiotoxic' in template_lower:\n",
    "        return 'cardiotoxicity'\n",
    "    elif 'nephrotoxic' in template_lower:\n",
    "        return 'nephrotoxicity'\n",
    "    elif 'hepatotoxic' in template_lower:\n",
    "        return 'hepatotoxicity'\n",
    "    elif 'neurotoxic' in template_lower:\n",
    "        return 'neurotoxicity'\n",
    "    elif 'myelosuppressive' in template_lower:\n",
    "        return 'myelosuppression'\n",
    "    elif 'ototoxic' in template_lower:\n",
    "        return 'ototoxicity'\n",
    "    \n",
    "    # 3. PHARMACOKINETIC INTERACTIONS\n",
    "    if 'metabolism' in template_lower:\n",
    "        return 'metabolism_change'\n",
    "    elif 'serum concentration' in template_lower:\n",
    "        if 'active metabolites' in template_lower:\n",
    "            return 'metabolite_concentration_change'\n",
    "        else:\n",
    "            return 'serum_concentration_change'\n",
    "    elif 'bioavailability' in template_lower:\n",
    "        return 'bioavailability_change'\n",
    "    elif 'absorption' in template_lower:\n",
    "        return 'absorption_change'\n",
    "    elif 'excretion rate' in template_lower:\n",
    "        return 'excretion_change'\n",
    "    elif 'protein binding' in template_lower:\n",
    "        return 'protein_binding_change'\n",
    "    \n",
    "    # 4. CARDIOVASCULAR EFFECTS\n",
    "    if 'qtc' in template_lower or 'qt' in template_lower:\n",
    "        return 'qtc_prolongation'\n",
    "    elif 'bradycardic' in template_lower:\n",
    "        return 'bradycardia'\n",
    "    elif 'tachycardic' in template_lower:\n",
    "        return 'tachycardia'\n",
    "    elif 'arrhythmogenic' in template_lower:\n",
    "        return 'arrhythmia'\n",
    "    elif 'hypotensive' in template_lower and 'orthostatic' not in template_lower:\n",
    "        return 'hypotension'\n",
    "    elif 'hypertensive' in template_lower:\n",
    "        return 'hypertension'\n",
    "    elif 'orthostatic hypotensive' in template_lower:\n",
    "        return 'orthostatic_hypotension'\n",
    "    elif 'antihypertensive' in template_lower:\n",
    "        return 'antihypertensive_effect'\n",
    "    elif 'vasoconstricting' in template_lower or 'vasopressor' in template_lower:\n",
    "        return 'vasoconstriction'\n",
    "    elif 'vasodilatory' in template_lower:\n",
    "        return 'vasodilation'\n",
    "    elif 'heart failure' in template_lower:\n",
    "        return 'heart_failure'\n",
    "    elif 'av block' in template_lower or 'atrioventricular' in template_lower:\n",
    "        return 'av_block'\n",
    "    \n",
    "    # 5. BLOOD/COAGULATION EFFECTS\n",
    "    if 'anticoagulant' in template_lower:\n",
    "        return 'anticoagulation'\n",
    "    elif 'antiplatelet' in template_lower:\n",
    "        return 'antiplatelet_effect'\n",
    "    elif 'bleeding' in template_lower:\n",
    "        return 'bleeding_risk'\n",
    "    elif 'thrombogenic' in template_lower:\n",
    "        return 'thrombosis'\n",
    "    \n",
    "    # 6. ELECTROLYTE EFFECTS\n",
    "    if 'hypokalemic' in template_lower:\n",
    "        return 'hypokalemia'\n",
    "    elif 'hyperkalemic' in template_lower or 'hyperkalemia' in template_lower:\n",
    "        return 'hyperkalemia'\n",
    "    elif 'hypocalcemic' in template_lower:\n",
    "        return 'hypocalcemia'\n",
    "    elif 'hypercalcemic' in template_lower:\n",
    "        return 'hypercalcemia'\n",
    "    elif 'hyponatremic' in template_lower:\n",
    "        return 'hyponatremia'\n",
    "    \n",
    "    # 7. CENTRAL NERVOUS SYSTEM\n",
    "    if 'cns depressant' in template_lower:\n",
    "        if 'hypertensive' in template_lower:\n",
    "            return 'cns_depression_and_hypertension'\n",
    "        elif 'hypotensive' in template_lower:\n",
    "            return 'cns_depression_and_hypotension'\n",
    "        else:\n",
    "            return 'cns_depression'\n",
    "    elif 'neuroexcitatory' in template_lower:\n",
    "        return 'neuroexcitation'\n",
    "    elif 'sedative' in template_lower:\n",
    "        return 'sedation'\n",
    "    elif 'central neurotoxic' in template_lower:\n",
    "        return 'central_neurotoxicity'\n",
    "    elif 'serotonergic' in template_lower:\n",
    "        return 'serotonergic_effect'\n",
    "    elif 'antipsychotic' in template_lower:\n",
    "        return 'antipsychotic_effect'\n",
    "    \n",
    "    # 8. METABOLIC EFFECTS\n",
    "    if 'hypoglycemic' in template_lower:\n",
    "        return 'hypoglycemia'\n",
    "    elif 'hyperglycemic' in template_lower:\n",
    "        return 'hyperglycemia'\n",
    "    \n",
    "    # 9. RESPIRATORY EFFECTS\n",
    "    if 'respiratory depressant' in template_lower:\n",
    "        return 'respiratory_depression'\n",
    "    elif 'bronchodilatory' in template_lower:\n",
    "        return 'bronchodilation'\n",
    "    elif 'bronchoconstrictory' in template_lower:\n",
    "        return 'bronchoconstriction'\n",
    "    \n",
    "    # 10. NEUROMUSCULAR EFFECTS\n",
    "    if 'neuromuscular blocking' in template_lower:\n",
    "        return 'neuromuscular_blockade'\n",
    "    elif 'adverse neuromuscular' in template_lower:\n",
    "        return 'adverse_neuromuscular'\n",
    "    elif 'myopathic rhabdomyolysis' in template_lower:\n",
    "        return 'rhabdomyolysis'\n",
    "    \n",
    "    # 11. OTHER PHARMACOLOGICAL EFFECTS\n",
    "    if 'therapeutic efficacy' in template_lower:\n",
    "        return 'therapeutic_efficacy'\n",
    "    elif 'analgesic' in template_lower:\n",
    "        return 'analgesic_effect'\n",
    "    elif 'anticholinergic' in template_lower:\n",
    "        return 'anticholinergic_effect'\n",
    "    elif 'immunosuppressive' in template_lower:\n",
    "        return 'immunosuppression'\n",
    "    elif 'diuretic' in template_lower:\n",
    "        return 'diuretic_effect'\n",
    "    elif 'stimulatory' in template_lower:\n",
    "        return 'stimulation'\n",
    "    \n",
    "    # 12. GASTROINTESTINAL\n",
    "    if 'ulcerogenic' in template_lower:\n",
    "        return 'ulcerogenic_effect'\n",
    "    elif 'constipating' in template_lower:\n",
    "        return 'constipation'\n",
    "    \n",
    "    # 13. FLUID/RENAL\n",
    "    if 'fluid retaining' in template_lower:\n",
    "        return 'fluid_retention'\n",
    "    \n",
    "    # 14. DERMATOLOGIC\n",
    "    if 'dermatologic' in template_lower:\n",
    "        return 'dermatologic_adverse'\n",
    "    \n",
    "    # 15. HYPERSENSITIVITY\n",
    "    if 'hypersensitivity' in template_lower:\n",
    "        return 'hypersensitivity'\n",
    "    \n",
    "    # 16. DIAGNOSTIC\n",
    "    if 'diagnostic agent' in template_lower:\n",
    "        return 'diagnostic_interference'\n",
    "    \n",
    "    # DEFAULT\n",
    "    return 'other_interaction'\n",
    "\n",
    "# First create templates (from previous code)\n",
    "def extract_interaction_template(description, drug1, drug2):\n",
    "    template = description\n",
    "    template = re.sub(re.escape(drug1), 'DRUG_A', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(re.escape(drug2), 'DRUG_B', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(r'\\bincreas(e|ed|es|ing)\\b', 'DIRECTION', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(r'\\bdecreas(e|ed|es|ing)\\b', 'DIRECTION', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(r'\\b(enhance|enhanced|enhances|enhancing)\\b', 'DIRECTION', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(r'\\b(reduc(e|ed|es|ing)|diminish(ed|es|ing)?|lower(ed|s|ing)?)\\b', 'DIRECTION', template, flags=re.IGNORECASE)\n",
    "    template = re.sub(r'\\b(elevat(e|ed|es|ing)|rais(e|ed|es|ing))\\b', 'DIRECTION', template, flags=re.IGNORECASE)\n",
    "    return template\n",
    "\n",
    "\n",
    "df = pd.read_csv('drug_interactions_cleaned.csv')\n",
    "print(f\"Loaded {len(df):,} interactions\")\n",
    "\n",
    "# Create templates\n",
    "df['Template'] = df.apply(\n",
    "    lambda row: extract_interaction_template(\n",
    "        row['Interaction Description'], \n",
    "        row['Drug 1'], \n",
    "        row['Drug 2']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "print(f\"Created {df['Template'].nunique()} unique templates\")\n",
    "\n",
    "# Extract direction from ORIGINAL description (before replacing with DIRECTION)\n",
    "df['Direction'] = df['Interaction Description'].apply(extract_direction)\n",
    "print(f\"\\nDirection distribution:\")\n",
    "print(df['Direction'].value_counts())\n",
    "\n",
    "# Categorize the templates\n",
    "df['Interaction_Category'] = df['Template'].apply(categorize_template)\n",
    "print(f\"\\nCreated {df['Interaction_Category'].nunique()} base categories\")\n",
    "\n",
    "# Combine category + direction\n",
    "df['Category_With_Direction'] = df['Interaction_Category'] + '_' + df['Direction']\n",
    "print(f\"Combined into {df['Category_With_Direction'].nunique()} final categories\")\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Interaction_Label'] = le.fit_transform(df['Category_With_Direction'])\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Number of classes for training: {num_classes}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nFINAL CLASS LABELS (with direction):\")\n",
    "print(\"=\"*80)\n",
    "for i, label in enumerate(le.classes_):\n",
    "    count = (df['Category_With_Direction'] == label).sum()\n",
    "    # Parse out category and direction for display\n",
    "    if '_increased' in label:\n",
    "        category = label.replace('_increased', '')\n",
    "        direction = '↑'\n",
    "    elif '_decreased' in label:\n",
    "        category = label.replace('_decreased', '')\n",
    "        direction = '↓'\n",
    "    elif '_neutral' in label:\n",
    "        category = label.replace('_neutral', '')\n",
    "        direction = '='\n",
    "    elif '_mixed' in label:\n",
    "        category = label.replace('_mixed', '')\n",
    "        direction = '±'\n",
    "    else:\n",
    "        category = label\n",
    "        direction = '?'\n",
    "    \n",
    "    print(f\"  {i:3d}. [{direction}] {category:.<45} {count:>6,} samples\")\n",
    "\n",
    "\n",
    "df.to_csv('drug_interactions_categorized.csv', index=False)\n",
    "print(f\"\\n✓ Saved to 'drug_interactions_categorized.csv'\")\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "print(\"✓ Saved label encoder\")\n",
    "\n",
    "# Save human-readable mapping\n",
    "with open('label_mapping.txt', 'w') as f:\n",
    "    f.write(\"LABEL MAPPING (Category + Direction)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        count = (df['Category_With_Direction'] == label).sum()\n",
    "        f.write(f\"{i:3d}. {label:<60} {count:>6,} samples\\n\")\n",
    "print(\"✓ Saved label mapping to 'label_mapping.txt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Next step: Run your fingerprint generation script!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique SMILES strings: 1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 456/1656 [00:00<00:01, 966.89it/s][20:44:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:44:24] WARNING: not removing hydrogen atom without neighbors\n",
      " 58%|█████▊    | 964/1656 [00:01<00:00, 995.02it/s] [20:44:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[20:44:24] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 1656/1656 [00:01<00:00, 966.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: 0 rows with failed fingerprints\n",
      "\n",
      "✓ Saved: drug_interactions_with_fingerprints.pkl\n",
      "Sample fingerprint shape: (2048,)\n"
     ]
    }
   ],
   "source": [
    "def smiles_to_fingerprint(smiles, radius=2, nBits=2048):\n",
    "    \"\"\"Convert SMILES to Morgan fingerprint\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "        fp = gen.GetFingerprint(mol)\n",
    "        return np.array(fp)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df = pd.read_csv('drug_interactions_categorized.csv')\n",
    "\n",
    "unique_smiles = pd.concat([df['Drug1_SMILES'], df['Drug2_SMILES']]).unique()\n",
    "print(f\"\\nUnique SMILES strings: {len(unique_smiles)}\")\n",
    "\n",
    "# unique SMILES to fingerprints\n",
    "smiles_to_fp_dict = {}\n",
    "for smiles in tqdm(unique_smiles):\n",
    "    fp = smiles_to_fingerprint(smiles)\n",
    "    smiles_to_fp_dict[smiles] = fp\n",
    "\n",
    "# fingerprints back to dataframe\n",
    "df['Drug1_FP'] = df['Drug1_SMILES'].map(smiles_to_fp_dict)\n",
    "df['Drug2_FP'] = df['Drug2_SMILES'].map(smiles_to_fp_dict)\n",
    "\n",
    "before_removal = len(df)\n",
    "df = df.dropna(subset=['Drug1_FP', 'Drug2_FP'])\n",
    "print(f\"Removed: {before_removal - len(df)} rows with failed fingerprints\")\n",
    "\n",
    "df.to_pickle('drug_interactions_with_fingerprints.pkl')\n",
    "print(\"\\n✓ Saved: drug_interactions_with_fingerprints.pkl\")\n",
    "\n",
    "print(f\"Sample fingerprint shape: {df.iloc[0]['Drug1_FP'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 189932 interactions\n",
      "Number of interaction types: 75\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('drug_interactions_with_fingerprints.pkl')\n",
    "print(f\"Dataset size: {len(df)} interactions\")\n",
    "\n",
    "# Load label encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Number of interaction types: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape: (189932, 2048)\n",
      "X2 shape: (189932, 2048)\n",
      "y shape: (189932,)\n",
      "\n",
      "Training set: 151945 samples\n",
      "Test set: 37987 samples\n"
     ]
    }
   ],
   "source": [
    "X1 = np.stack(df['Drug1_FP'].values).astype(np.float32)  # Shape: (n_samples, 2048)\n",
    "X2 = np.stack(df['Drug2_FP'].values).astype(np.float32)  # Shape: (n_samples, 2048)\n",
    "y = df['Interaction_Label'].values.astype(np.int64)\n",
    "\n",
    "print(f\"X1 shape: {X1.shape}\")\n",
    "print(f\"X2 shape: {X2.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Split data\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X1_train)} samples\")\n",
    "print(f\"Test set: {len(X1_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 2375\n",
      "Number of test batches: 594\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class DDIDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Drug-Drug Interactions\"\"\"\n",
    "    def __init__(self, drug1_fps, drug2_fps, labels):\n",
    "        self.drug1_fps = torch.FloatTensor(drug1_fps)\n",
    "        self.drug2_fps = torch.FloatTensor(drug2_fps)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.drug1_fps[idx], self.drug2_fps[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DDIDataset(X1_train, X2_train, y_train)\n",
    "test_dataset = DDIDataset(X1_test, X2_test, y_test)\n",
    "\n",
    "# -------------------------------\n",
    "# ADD WEIGHTED SAMPLER HERE\n",
    "# -------------------------------\n",
    "class_counts = torch.bincount(torch.tensor(y_train))          # count samples per class\n",
    "class_weights = 1.0 / (class_counts + 1e-8)                   # avoid div-by-zero\n",
    "sample_weights = class_weights[y_train]                       # weight each training sample\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Replace shuffle=True with the sampler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "# Test loader stays the same\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIPredictor(nn.Module):\n",
    "    \"\"\"CNN model for Drug-Drug Interaction prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, fp_size=2048, num_classes=100):\n",
    "        super(DDIPredictor, self).__init__()\n",
    "        \n",
    "        # Drug encoder (shared for both drugs)\n",
    "        self.drug_encoder = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(1, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.AdaptiveMaxPool1d(1)  # Global max pooling\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 1024),  # *2 because we concatenate two drugs\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1, drug2):\n",
    "        # Reshape for Conv1d: (batch, channels, length)\n",
    "        drug1 = drug1.unsqueeze(1)  # (batch, 1, 2048)\n",
    "        drug2 = drug2.unsqueeze(1)  # (batch, 1, 2048)\n",
    "        \n",
    "        # Encode both drugs\n",
    "        drug1_features = self.drug_encoder(drug1).squeeze(-1)  # (batch, 512)\n",
    "        drug2_features = self.drug_encoder(drug2).squeeze(-1)  # (batch, 512)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([drug1_features, drug2_features], dim=1)  # (batch, 1024)\n",
    "        \n",
    "        # Pass through FC layers\n",
    "        output = self.fc_layers(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "model = DDIPredictor(fp_size=2048, num_classes=num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (N, C)\n",
    "        # targets: (N)\n",
    "\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits,\n",
    "            targets,\n",
    "            reduction='none',\n",
    "            weight=self.alpha\n",
    "        )\n",
    "\n",
    "        pt = torch.exp(-ce_loss)  # probability of the true class\n",
    "\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training setup complete\n",
      "Optimizer: Adam\n",
      "Loss function: FocalLoss\n",
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "print(\"✓ Training setup complete\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Loss function: {criterion.__class__.__name__}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for drug1, drug2, labels in pbar:\n",
    "        # Move to device\n",
    "        drug1, drug2, labels = drug1.to(device), drug2.to(device), labels.to(device)\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(drug1, drug2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * drug1.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for drug1, drug2, labels in loader:\n",
    "            drug1, drug2, labels = drug1.to(device), drug2.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(drug1, drug2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * drug1.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:46<00:00, 14.26it/s, loss=3.39, acc=0.0831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] LR: 0.001000\n",
      "  Train Loss: 3.4862 | Train Acc: 0.0831\n",
      "  Val Loss:   3.6193 | Val Acc:   0.0115\n",
      "  ✓ New best model saved! (Val Acc: 0.0115)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.18it/s, loss=2.57, acc=0.106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] LR: 0.001000\n",
      "  Train Loss: 3.3058 | Train Acc: 0.1063\n",
      "  Val Loss:   3.4120 | Val Acc:   0.0091\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.11it/s, loss=3.48, acc=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] LR: 0.001000\n",
      "  Train Loss: 3.1967 | Train Acc: 0.1220\n",
      "  Val Loss:   3.4407 | Val Acc:   0.0086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.08it/s, loss=2.66, acc=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] LR: 0.001000\n",
      "  Train Loss: 3.1231 | Train Acc: 0.1353\n",
      "  Val Loss:   3.4710 | Val Acc:   0.0071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:46<00:00, 14.23it/s, loss=2.55, acc=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] LR: 0.001000\n",
      "  Train Loss: 3.0678 | Train Acc: 0.1436\n",
      "  Val Loss:   3.6942 | Val Acc:   0.0049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:45<00:00, 14.39it/s, loss=3.5, acc=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Learning rate reduced: 0.001000 -> 0.000500\n",
      "Epoch [6/100] LR: 0.001000\n",
      "  Train Loss: 2.9902 | Train Acc: 0.1557\n",
      "  Val Loss:   3.8681 | Val Acc:   0.0063\n",
      "\n",
      "Early stopping triggered after 6 epochs\n",
      "\n",
      "✓ Training complete!\n",
      "Best validation accuracy: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18605\\AppData\\Local\\Temp\\ipykernel_19128\\1735389185.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_ddi_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         8\n",
      "           1     0.0000    0.0000    0.0000     11972\n",
      "           2     0.0000    0.0000    0.0000        15\n",
      "           3     0.0000    0.0000    0.0000         4\n",
      "           4     0.0000    0.0000    0.0000        54\n",
      "           5     0.0000    0.0000    0.0000        68\n",
      "           6     0.0121    0.6538    0.0238        52\n",
      "           7     0.0333    0.0905    0.0486       630\n",
      "           8     0.0000    0.0000    0.0000         1\n",
      "           9     0.0031    0.1127    0.0060        71\n",
      "          10     0.0000    0.0000    0.0000        19\n",
      "          11     0.5000    0.0143    0.0278        70\n",
      "          12     0.0000    0.0000    0.0000       132\n",
      "          13     0.0271    0.4255    0.0510        94\n",
      "          14     0.0000    0.0000    0.0000         2\n",
      "          15     0.0000    0.0000    0.0000        27\n",
      "          16     0.0000    0.0000    0.0000       238\n",
      "          17     0.0000    0.0000    0.0000         6\n",
      "          18     0.0107    0.1000    0.0193        60\n",
      "          19     0.0000    0.0000    0.0000       217\n",
      "          20     0.0000    0.0000    0.0000        41\n",
      "          21     0.0000    0.0000    0.0000      1087\n",
      "          22     0.0000    0.0000    0.0000        35\n",
      "          23     0.0000    0.0000    0.0000         3\n",
      "          24     0.0000    0.0000    0.0000         8\n",
      "          25     0.0082    0.2083    0.0157        72\n",
      "          26     0.0145    0.0167    0.0155       360\n",
      "          27     0.0000    0.0000    0.0000         6\n",
      "          28     0.0019    0.0109    0.0032        92\n",
      "          29     0.0000    0.0000    0.0000         2\n",
      "          30     0.0003    0.0588    0.0007        17\n",
      "          31     0.0000    0.0000    0.0000        17\n",
      "          32     0.0015    1.0000    0.0031         7\n",
      "          33     0.0000    0.0000    0.0000        64\n",
      "          34     0.0000    0.0000    0.0000         4\n",
      "          35     0.0245    0.1547    0.0422       627\n",
      "          36     0.0000    0.0000    0.0000       259\n",
      "          37     0.0000    0.0000    0.0000        33\n",
      "          38     0.0230    0.0649    0.0340       416\n",
      "          39     0.0000    0.0000    0.0000       219\n",
      "          40     0.0000    0.0000    0.0000        22\n",
      "          41     0.0000    0.0000    0.0000      1693\n",
      "          42     0.0000    0.0000    0.0000        58\n",
      "          43     0.0000    0.0000    0.0000      6986\n",
      "          44     0.0000    0.0000    0.0000      1020\n",
      "          45     0.0000    0.0000    0.0000        59\n",
      "          46     0.0000    0.0000    0.0000       101\n",
      "          47     0.0000    0.0000    0.0000         3\n",
      "          48     0.0000    0.0000    0.0000       138\n",
      "          49     0.0084    0.3627    0.0164       204\n",
      "          50     0.0000    0.0000    0.0000        18\n",
      "          51     0.3462    0.3333    0.3396        81\n",
      "          52     0.0000    0.0000    0.0000        15\n",
      "          53     0.0000    0.0000    0.0000       109\n",
      "          54     0.0000    0.0000    0.0000        10\n",
      "          55     0.0000    0.0000    0.0000         5\n",
      "          56     0.0000    0.0000    0.0000         1\n",
      "          57     0.0000    0.0000    0.0000      1259\n",
      "          58     0.0000    0.0000    0.0000        57\n",
      "          59     0.0000    0.0000    0.0000        10\n",
      "          60     0.1111    0.0093    0.0172       107\n",
      "          61     0.1488    0.0928    0.1143       194\n",
      "          62     0.0000    0.0000    0.0000       158\n",
      "          63     0.0000    0.0000    0.0000      2081\n",
      "          64     0.0000    0.0000    0.0000      4634\n",
      "          65     0.0000    0.0000    0.0000        99\n",
      "          66     0.0051    0.2000    0.0100         5\n",
      "          67     0.0050    0.1408    0.0096        71\n",
      "          68     0.0000    0.0000    0.0000      1506\n",
      "          69     0.0032    0.0208    0.0055        48\n",
      "          70     0.0000    0.0000    0.0000        21\n",
      "          71     0.0000    0.0000    0.0000         6\n",
      "          72     0.0000    0.0000    0.0000        57\n",
      "          73     0.0044    0.1667    0.0085        36\n",
      "          74     0.0000    0.0000    0.0000         6\n",
      "\n",
      "    accuracy                         0.0115     37987\n",
      "   macro avg     0.0172    0.0565    0.0108     37987\n",
      "weighted avg     0.0043    0.0115    0.0038     37987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 100\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# History\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate (use test set as validation)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Check if LR was reduced\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"  Learning rate reduced: {old_lr:.6f} -> {new_lr:.6f}\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] LR: {current_lr:.6f}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_ddi_model.pth')\n",
    "        print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_ddi_model.pth'))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for drug1, drug2, labels in test_loader:\n",
    "        drug1, drug2 = drug1.to(device), drug2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(drug1, drug2)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[0].plot(history['val_acc'], label='Val Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history['train_loss'], label='Train Loss')\n",
    "axes[1].plot(history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_epoch = np.argmax(history['val_acc'])\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation accuracy: {history['val_acc'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set with best model\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_fingerprint(smiles, radius=2, nBits=2048):\n",
    "    \"\"\"Convert SMILES to Morgan fingerprint\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "        fp = gen.GetFingerprint(mol)\n",
    "        return np.array(fp, dtype=np.float32)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def predict_interaction(drug1_smiles, drug2_smiles, top_k=3):\n",
    "    \"\"\"Predict interaction between two drugs\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to fingerprints\n",
    "    fp1 = smiles_to_fingerprint(drug1_smiles)\n",
    "    fp2 = smiles_to_fingerprint(drug2_smiles)\n",
    "    \n",
    "    if fp1 is None or fp2 is None:\n",
    "        return [(\"Invalid SMILES\", 0.0)]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    fp1 = torch.FloatTensor(fp1).unsqueeze(0).to(device)\n",
    "    fp2 = torch.FloatTensor(fp2).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(fp1, fp2)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        interaction_type = le.inverse_transform([idx.item()])[0]\n",
    "        results.append((interaction_type, prob.item()))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print(\"Testing predictions on random samples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_samples = df.sample(5, random_state=42)\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Drug 1: {row['Drug 1']}\")\n",
    "    print(f\"Drug 2: {row['Drug 2']}\")\n",
    "    print(f\"\\nActual Interaction:\")\n",
    "    print(f\"  {row['Interaction Description']}\")\n",
    "    \n",
    "    predictions = predict_interaction(row['Drug1_SMILES'], row['Drug2_SMILES'], top_k=3)\n",
    "    \n",
    "    print(f\"\\nTop 3 Predictions:\")\n",
    "    for i, (interaction, conf) in enumerate(predictions, 1):\n",
    "        match = \"✓\" if interaction == row['Interaction Description'] else \"✗\"\n",
    "        print(f\"  {i}. [{match}] {interaction}\")\n",
    "        print(f\"      Confidence: {conf:.2%}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
