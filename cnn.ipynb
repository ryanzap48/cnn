{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pubchempy as pcp\n",
    "import re\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: NVIDIA GeForce RTX 2070 SUPER\n",
      "GPU memory: 8.00 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_direction(description):\n",
    "    desc = description.lower()\n",
    "    if re.search(r\"\\bincreas(e|ed)\\b\", desc):\n",
    "        return \"increased\"\n",
    "    \n",
    "    if re.search(r\"\\bdecreas(e|ed)\\b|\\breduced\\b\", desc):\n",
    "        return \"decreased\"\n",
    "    \n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction\n",
      "increased    128533\n",
      "decreased     61399\n",
      "Name: count, dtype: int64\n",
      "Number of classes for training: 2\n"
     ]
    }
   ],
   "source": [
    "# add interaction label to dataset\n",
    "df = pd.read_csv('drug_interactions_cleaned.csv')\n",
    "\n",
    "df['Direction'] = df['Interaction Description'].apply(extract_direction)\n",
    "print(df['Direction'].value_counts())\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Interaction_Label'] = le.fit_transform(df['Direction'])\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Number of classes for training: {num_classes}\")\n",
    "\n",
    "df.to_csv('drug_interactions_direction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_fingerprint(smiles, radius=2, nBits=2048):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "        fp = gen.GetFingerprint(mol)\n",
    "        return np.array(fp)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking loaded data:\n",
      "Direction\n",
      "increased    128533\n",
      "decreased     61399\n",
      "Name: count, dtype: int64\n",
      "Interaction_Label\n",
      "1    128533\n",
      "0     61399\n",
      "Name: count, dtype: int64\n",
      "Unique labels: [0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 485/1656 [00:00<00:01, 979.87it/s][18:25:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:25:24] WARNING: not removing hydrogen atom without neighbors\n",
      " 60%|██████    | 997/1656 [00:01<00:00, 1009.00it/s][18:25:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:25:25] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 1656/1656 [00:01<00:00, 985.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: 0 rows with failed fingerprints\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('drug_interactions_direction.csv')\n",
    "print(\"Checking loaded data:\")\n",
    "print(df['Direction'].value_counts())\n",
    "print(df['Interaction_Label'].value_counts())\n",
    "print(f\"Unique labels: {df['Interaction_Label'].unique()}\")\n",
    "\n",
    "unique_smiles = pd.concat([df['Drug1_SMILES'], df['Drug2_SMILES']]).unique()\n",
    "\n",
    "smiles_to_fp_dict = {}\n",
    "for smiles in tqdm(unique_smiles):\n",
    "    fp = smiles_to_fingerprint(smiles)\n",
    "    smiles_to_fp_dict[smiles] = fp\n",
    "\n",
    "df['Drug1_FP'] = df['Drug1_SMILES'].map(smiles_to_fp_dict)\n",
    "df['Drug2_FP'] = df['Drug2_SMILES'].map(smiles_to_fp_dict)\n",
    "\n",
    "before_removal = len(df)\n",
    "df = df.dropna(subset=['Drug1_FP', 'Drug2_FP'])\n",
    "print(f\"Removed: {before_removal - len(df)} rows with failed fingerprints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Drug1_FP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18605\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Drug1_FP'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X1 = np.stack(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDrug1_FP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.values).astype(np.float32)\n\u001b[32m      2\u001b[39m X2 = np.stack(df[\u001b[33m'\u001b[39m\u001b[33mDrug2_FP\u001b[39m\u001b[33m'\u001b[39m].values).astype(np.float32)\n\u001b[32m      3\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mInteraction_Label\u001b[39m\u001b[33m'\u001b[39m].values.astype(np.int64)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18605\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\18605\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Drug1_FP'"
     ]
    }
   ],
   "source": [
    "X1 = np.stack(df['Drug1_FP'].values).astype(np.float32)\n",
    "X2 = np.stack(df['Drug2_FP'].values).astype(np.float32)\n",
    "y = df['Interaction_Label'].values.astype(np.int64)\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drug1_fps[idx], \u001b[38;5;28mself\u001b[39m.drug2_fps[idx], \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_dataset = DDIDataset(\u001b[43mX1_train\u001b[49m, X2_train, y_train)\n\u001b[32m     14\u001b[39m test_dataset = DDIDataset(X1_test, X2_test, y_test)\n\u001b[32m     16\u001b[39m batch_size = \u001b[32m64\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'X1_train' is not defined"
     ]
    }
   ],
   "source": [
    "class DDIDataset(Dataset):\n",
    "    def __init__(self, drug1_fps, drug2_fps, labels):\n",
    "        self.drug1_fps = torch.FloatTensor(drug1_fps)\n",
    "        self.drug2_fps = torch.FloatTensor(drug2_fps)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.drug1_fps[idx], self.drug2_fps[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = DDIDataset(X1_train, X2_train, y_train)\n",
    "test_dataset = DDIDataset(X1_test, X2_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIPredictor(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DDIPredictor, self).__init__()\n",
    "        \n",
    "        self.drug_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=8, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=8, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=8, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=8, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1, drug2):\n",
    "        drug1 = drug1.unsqueeze(1)  # (batch, 1, 2048)\n",
    "        drug2 = drug2.unsqueeze(1)  # (batch, 1, 2048)\n",
    "    \n",
    "        drug1_features = self.drug_encoder(drug1).squeeze(-1)  # (batch, 512)\n",
    "        drug2_features = self.drug_encoder(drug2).squeeze(-1)  # (batch, 512)\n",
    "        \n",
    "        combined = torch.cat([drug1_features, drug2_features], dim=1)\n",
    "        output = self.fc_layers(combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "model = DDIPredictor(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for drug1, drug2, labels in pbar:\n",
    "        drug1, drug2, labels = drug1.to(device), drug2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(drug1, drug2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * drug1.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for drug1, drug2, labels in loader:\n",
    "            drug1, drug2, labels = drug1.to(device), drug2.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(drug1, drug2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * drug1.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:57<00:00, 13.35it/s, loss=0.253, acc=0.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] LR: 0.001000\n",
      "  Train Loss: 0.4307 | Train Acc: 0.8003\n",
      "  Val Loss:   0.3210 | Val Acc:   0.8613\n",
      "New best model saved (Val Acc: 0.8613)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:53<00:00, 13.70it/s, loss=0.31, acc=0.872] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] LR: 0.001000\n",
      "  Train Loss: 0.2961 | Train Acc: 0.8717\n",
      "  Val Loss:   0.2416 | Val Acc:   0.8968\n",
      "New best model saved (Val Acc: 0.8968)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:51<00:00, 13.89it/s, loss=0.372, acc=0.891] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] LR: 0.001000\n",
      "  Train Loss: 0.2526 | Train Acc: 0.8914\n",
      "  Val Loss:   0.2515 | Val Acc:   0.9042\n",
      "New best model saved (Val Acc: 0.9042)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:50<00:00, 13.92it/s, loss=0.112, acc=0.902] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] LR: 0.001000\n",
      "  Train Loss: 0.2299 | Train Acc: 0.9018\n",
      "  Val Loss:   0.3516 | Val Acc:   0.9148\n",
      "New best model saved (Val Acc: 0.9148)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:49<00:00, 14.02it/s, loss=0.258, acc=0.91]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] LR: 0.001000\n",
      "  Train Loss: 0.2110 | Train Acc: 0.9104\n",
      "  Val Loss:   0.2170 | Val Acc:   0.9173\n",
      "New best model saved (Val Acc: 0.9173)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.19it/s, loss=0.37, acc=0.914]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] LR: 0.001000\n",
      "  Train Loss: 0.2020 | Train Acc: 0.9145\n",
      "  Val Loss:   1.5900 | Val Acc:   0.9248\n",
      "New best model saved (Val Acc: 0.9248)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.18it/s, loss=0.219, acc=0.916] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] LR: 0.001000\n",
      "  Train Loss: 0.1996 | Train Acc: 0.9159\n",
      "  Val Loss:   0.5119 | Val Acc:   0.9269\n",
      "New best model saved (Val Acc: 0.9269)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.11it/s, loss=0.103, acc=0.921] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] LR: 0.001000\n",
      "  Train Loss: 0.1868 | Train Acc: 0.9205\n",
      "  Val Loss:   1.6963 | Val Acc:   0.9354\n",
      "New best model saved (Val Acc: 0.9354)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.21it/s, loss=0.0427, acc=0.923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] LR: 0.001000\n",
      "  Train Loss: 0.1853 | Train Acc: 0.9227\n",
      "  Val Loss:   1.8053 | Val Acc:   0.9355\n",
      "New best model saved (Val Acc: 0.9355)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.22it/s, loss=0.66, acc=0.925]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] LR: 0.001000\n",
      "  Train Loss: 0.1814 | Train Acc: 0.9248\n",
      "  Val Loss:   0.7566 | Val Acc:   0.9330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:46<00:00, 14.27it/s, loss=0.228, acc=0.924] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] LR: 0.001000\n",
      "  Train Loss: 0.1820 | Train Acc: 0.9240\n",
      "  Val Loss:   0.1705 | Val Acc:   0.9376\n",
      "New best model saved (Val Acc: 0.9376)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.13it/s, loss=0.106, acc=0.926] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] LR: 0.001000\n",
      "  Train Loss: 0.1776 | Train Acc: 0.9260\n",
      "  Val Loss:   0.1457 | Val Acc:   0.9389\n",
      "New best model saved (Val Acc: 0.9389)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.15it/s, loss=0.401, acc=0.926] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] LR: 0.001000\n",
      "  Train Loss: 0.1755 | Train Acc: 0.9265\n",
      "  Val Loss:   0.1711 | Val Acc:   0.9414\n",
      "New best model saved (Val Acc: 0.9414)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:47<00:00, 14.16it/s, loss=0.0213, acc=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] LR: 0.001000\n",
      "  Train Loss: 0.1685 | Train Acc: 0.9293\n",
      "  Val Loss:   0.1689 | Val Acc:   0.9370\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.11it/s, loss=0.167, acc=0.931] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] LR: 0.001000\n",
      "  Train Loss: 0.1646 | Train Acc: 0.9312\n",
      "  Val Loss:   0.1472 | Val Acc:   0.9417\n",
      "New best model saved (Val Acc: 0.9417)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.09it/s, loss=0.121, acc=0.933] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] LR: 0.001000\n",
      "  Train Loss: 0.1617 | Train Acc: 0.9330\n",
      "  Val Loss:   0.1381 | Val Acc:   0.9442\n",
      "New best model saved (Val Acc: 0.9442)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.09it/s, loss=0.436, acc=0.932] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] LR: 0.001000\n",
      "  Train Loss: 0.1648 | Train Acc: 0.9317\n",
      "  Val Loss:   0.7761 | Val Acc:   0.9386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:49<00:00, 14.03it/s, loss=0.272, acc=0.931] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] LR: 0.001000\n",
      "  Train Loss: 0.1639 | Train Acc: 0.9309\n",
      "  Val Loss:   0.1835 | Val Acc:   0.9427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.10it/s, loss=0.382, acc=0.932] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] LR: 0.001000\n",
      "  Train Loss: 0.1637 | Train Acc: 0.9320\n",
      "  Val Loss:   0.7000 | Val Acc:   0.9439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2375/2375 [02:48<00:00, 14.13it/s, loss=0.279, acc=0.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] LR: 0.001000\n",
      "  Train Loss: 0.1699 | Train Acc: 0.9296\n",
      "  Val Loss:   7.7514 | Val Acc:   0.9448\n",
      "New best model saved (Val Acc: 0.9448)\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Best validation accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(f\"New best model saved (Val Acc: {val_acc:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m fig, axes = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m axes[\u001b[32m0\u001b[39m].plot(\u001b[43mhistory\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTrain Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m axes[\u001b[32m0\u001b[39m].plot(history[\u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mVal Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m axes[\u001b[32m0\u001b[39m].set_title(\u001b[33m'\u001b[39m\u001b[33mModel Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[0].plot(history['val_acc'], label='Val Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history['train_loss'], label='Train Loss')\n",
    "axes[1].plot(history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_epoch = np.argmax(history['val_acc'])\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation accuracy: {history['val_acc'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_interaction(drug1_smiles, drug2_smiles):\n",
    "    model.eval()\n",
    "    \n",
    "    fp1 = smiles_to_fingerprint(drug1_smiles)\n",
    "    fp2 = smiles_to_fingerprint(drug2_smiles)\n",
    "    \n",
    "    fp1 = torch.FloatTensor(fp1).unsqueeze(0).to(device)\n",
    "    fp2 = torch.FloatTensor(fp2).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(fp1, fp2)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(probabilities, 2)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        interaction_type = le.inverse_transform([idx.item()])[0]\n",
    "        results.append((interaction_type, prob.item()))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles_robust(drug_name):\n",
    "    results = pcp.get_compounds(drug_name, 'name')\n",
    "    if results:\n",
    "        compound = results[0]\n",
    "        return compound.canonical_smiles\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_drugs(drug_name1, drug_name2):\n",
    "    smiles1 = get_smiles_robust(drug_name1)\n",
    "    smiles2 = get_smiles_robust(drug_name2)\n",
    "    \n",
    "    if smiles1 is None:\n",
    "        print(f\"Could not find SMILES for drug: {drug_name1}\")\n",
    "        return []\n",
    "    if smiles2 is None:\n",
    "        print(f\"Could not find SMILES for drug: {drug_name2}\")\n",
    "        return []\n",
    "    \n",
    "    return predict_interaction(smiles1, smiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18605\\AppData\\Local\\Temp\\ipykernel_18828\\2810630444.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
      "C:\\Users\\18605\\AppData\\Local\\Temp\\ipykernel_18828\\4028118897.py:5: PubChemPyDeprecationWarning: canonical_smiles is deprecated: Use connectivity_smiles instead\n",
      "  return compound.canonical_smiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: decreased\n",
      "      Confidence: 94.69%\n",
      "Interaction: increased\n",
      "      Confidence: 5.31%\n"
     ]
    }
   ],
   "source": [
    "model = DDIPredictor()\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "prediction = get_predictions_for_drugs('Nimesulide', 'Etacrynic acid')\n",
    "\n",
    "for i, (interaction, conf) in enumerate(prediction, 1):\n",
    "    print(f\"Interaction: {interaction}\")\n",
    "    print(f\"      Confidence: {conf:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
